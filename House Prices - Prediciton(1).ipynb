{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Tasks:\n### Eda\nThe main tasks to do in Exploratory Data Analysis:\n* Null vars by column\n* Evaluate id, for duplicated cases.\n* Describe our target var, quantils, standard deviation(S),avg,min,max,distribution, bucketized.\n* Describe categorical features: each category how is related to avg metric(*some categorical features could be changed to numerical ones -i.e. Excelent, medium,bad - in order to avoid make binary poolQuality*).\n* Describe numerical features: distribution(*relation with target? in same graph*), covariance with targeted var.     \n\n### Modeling\nYou should complete this !!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from edautils import check_duplicates,fig_axes_subplots,set_dark\nset_dark()\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"ticks\", context=\"notebook\")\n#plt.style.use(\"dark_background\")\n\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_rows\", 90)\ntrain.head(3).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET = 'SalePrice'\nID = 'Id'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check Null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nnulls = train.isnull().sum().sort_values(ascending=False)\nnulls = nulls[nulls > 0]\ng = sns.barplot(x=nulls.index,y=nulls.values)\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In order to train a model es mandatory fix this null values. We have some columns with over than 90% of null values!, in some cases is a reference to a new feature, i.e., PoolQC is the Pool Quality if you don't have a Pool you have a null value in this. So the new feature can be, Has a Pool?."},{"metadata":{"trusted":true},"cell_type":"code","source":"check_duplicates(train[ID])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating Target data\nWe need to understand our target data, because all the project are related to this. What we look for? specifically nothing, but we need understand very well this, because when we analyse some other features, we will want to correlate with this."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[TARGET].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[TARGET].hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" g = sns.boxplot(x=train[TARGET])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluating Categorical columns:\nWe have ~90 features columns, we want to understand how much are categorical, and if this \"categories\" are related in some way with our target var. Additionally we want to think about the best representation in our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = list(train.select_dtypes(include='object').columns)\nprint(categorical_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def describe_categorical(dataframe,category):\n    d = dataframe.groupby(category).agg({ID:['count'],TARGET:['mean','std']})\n    d.columns = d.columns.get_level_values(1).astype(str)\n    d['share'] = round(100*d['count']/d['count'].sum(),2)\n    #d['share'] = d['share'].map('{:.2f}'.format)\n    d['mean'] = d['mean'].map('{:,.2f}'.format)\n    d['std'] = d['std'].map('{:,.2f}'.format)\n    \n    return d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[categorical_cols] = train[categorical_cols].fillna(\"--NAN--\")\nfor c in categorical_cols:\n    print(describe_categorical(train,c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def describe_categorical_graph(df, cols, fig_size=(18,20)):\n    # Used in next graphs to print the same order inner category options\n    order = {}\n    \n    fig, axs = plt.subplots(len(cols), 2, sharey=False, figsize=fig_size)\n    \n    def boxplot(df,col,target,ax,order=None):\n        g = sns.boxplot(x=col, y=target, ax=axs[ic,0], data=df, order=order)#, order=order[c])\n        g.set_xlabel(\"\")\n        return g\n    \n    def barplot(df,col,ax):\n        ys = list(df.share.values)\n        xs = list(df.index.values)\n        g = sns.barplot(x=xs, y=ys, ax=ax)\n    \n        for ix,y in enumerate(ys):\n            col_text = f'{df[\"count\"].values[ix]}\\n{round(ys[ix],1)}%'\n            g.text(ix, y+0.01, col_text, color='black', ha=\"center\", fontsize=10)\n        return g\n    \n    for ic,c in enumerate(cols):\n        d = describe_categorical(df,c).sort_values(by='share',ascending=False)\n        g0 = boxplot(df,c,TARGET,axs[ic,0], order=d.index.values)\n        g0.set_xticklabels(g0.get_xticklabels(), rotation=90, horizontalalignment='right')\n        g0.set_title(c)\n        g1 = barplot(d, c, axs[ic,1])\n        g1.set_xticklabels(g1.get_xticklabels(), rotation=90, horizontalalignment='right')\n        \n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"describe_categorical_graph(train, categorical_cols, (16,150))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting things about the graphs:\n* Looking our statistics(Quartiles,Avg,etc) for each categorical column, we could see the category impact in our target var(Sale Price). And in general all our variables are related in some way with that.\n* But seeing the category share in each column, in specific categories,  all data are concentrated in just one value. One approach is just use categories with less than 95% of share in one value(over than ~70 cases in other values)."},{"metadata":{},"cell_type":"markdown","source":"### Evaluating Numerical columns:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols = [c for c in train.columns if c not in categorical_cols]\nnumerical_cols.remove(ID)\nnumerical_cols.remove(TARGET)\nprint(numerical_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in numerical_cols:\n    train.plot(kind='scatter', x=c, y=TARGET)#, s=df.col3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ncorr_matrix = train[numerical_cols+[TARGET]].corr().abs()\nsns.heatmap(corr_matrix, vmin=0, vmax=1)#.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#corm.SalePrice[np.logical_or(corm.SalePrice > 0.2, corm.SalePrice < -0.2)]\ncorr_matrix.SalePrice[corr_matrix.SalePrice.abs() < 0.2].abs().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{},"cell_type":"markdown","source":"### Encoding Categorical variables\nWe will encode categorical variables with ordinal values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_mean_encoder(frame, feature):\n    ordering = frame[[feature,TARGET]].groupby(feature).mean()\n    ordering = ordering.sort_values(by=TARGET, ascending=True)\n    ordering = ordering.reset_index()\n    ordering.index = ordering.index+1\n    feature_cateogory = ordering[feature].to_dict()\n\n    for order,category in feature_cateogory.items():\n        frame.loc[frame[feature] == category, 'E_'+feature] = order\n\nfor col in categorical_cols:\n    linear_mean_encoder(train,col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Solving Null values in numerical columns\nWe will solve null values looking for most correlated column, and using it in order to regression missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_null = train[numerical_cols].isnull().sum()\nnumerical_null = numerical_null[numerical_null>0].sort_values(ascending=False)\nnumerical_null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\ndef columns_best_descriptors(data, columns_to_describe, avoided_columns =[]):\n    numeric_null_correlations = data.corr()[columns_to_describe].abs()\n    descriptors = {}\n    for c in numeric_null_correlations.columns:\n        descriptors_category =  numeric_null_correlations[c].sort_values(ascending=False)\n        avoided_columns = list(numerical_null.index)+[c]\n        descriptors[c]=str(descriptors_category[~descriptors_category.index.isin(avoided_columns)].head(1).index.values[0])\n    return descriptors\n\ndescriptors = columns_best_descriptors(data = train, \n                                       columns_to_describe = list(numerical_null.index.values), \n                                       avoided_columns = list(numerical_null.index.values))\n\nprint(descriptors)\ndef simple_linear_models_with_dict(train, y_x_dict_columns):\n    models = {}\n    for y_column, x_column in y_x_dict_columns.items():\n        d = train[~train[y_column].isnull()]\n        y = d[y_column]\n        x = d[x_column].to_numpy().reshape(-1,1)\n        models[y_column] = linear_model.LinearRegression()\n        models[y_column].fit(x, y)\n    return models\n\nmodels = simple_linear_models_with_dict(train, descriptors)\n\nfor y_column,x_column in descriptors.items():\n    synthetic_col = \"S_\"+y_column\n    train[synthetic_col] = train[y_column]\n    for ix, r in train[train[synthetic_col].isnull()].iterrows():\n        syntetic_value = models[y_column].predict([[r[x_column]]])\n        train.at[ix,synthetic_col] = syntetic_value\n    print(f\"Synthetic col: {synthetic_col} was created successfully for {y_column}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking syntetic values\nWe will make some basics checks, i.e, check some statistics about the distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['S_LotFrontage'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['LotFrontage','S_LotFrontage']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training models"},{"metadata":{},"cell_type":"markdown","source":"Removing columns with nulls (we have synthetized ones), and categorical ones(we have encoded ones)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.copy()\nY = train[TARGET].copy()\nX.drop(labels=[ID,TARGET], axis=1, inplace=True)\nX.drop(labels=list(descriptors.keys()), axis=1, inplace=True)\nX.drop(labels=categorical_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=10) \nprint(f\"train size : {len(y_train)}\")\nprint(f\"test size : {len(y_test)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(x_train.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selecting our model\nWe want to select the best model to predict our price, but depending about our organization, we will select our cost function. I.e, if our company loss money if we have a bad prediction in large amounts, maybe we will use a mean_squared_error to select the best one. But if we need to find the best model representation without focus in large amount \"squared r\" is a better option."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nparameters = {'normalize':[True,False],'fit_intercept':[True,False]}\nmodel = linear_model.Ridge()\nreg = GridSearchCV(model, parameters, cv=5, scoring='r2')\nreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error,r2_score\n\npreds = reg.best_estimator_.predict(x_test)\n\nprint('Mean squared error: %.2f' % mean_squared_error(y_test, preds))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.4f' % r2_score(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nreg = DecisionTreeRegressor(random_state = 0)\nreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = reg.predict(x_test)\nprint('Mean squared error: %.2f' % mean_squared_error(y_test, preds))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.4f' % r2_score(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}